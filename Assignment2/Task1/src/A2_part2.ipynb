{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import simplejson as json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings\n",
    "word2vec = api.load(\"word2vec-google-news-300\")\n",
    "glove = api.load(\"glove-wiki-gigaword-100\")\n",
    "fasttext = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class SequenceTaggingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['text'], self.data[idx]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vanilla RNN model\n",
    "class VanillaRNNModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, output_size):\n",
    "        super(VanillaRNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.rnn = nn.RNN(embedding_matrix.shape[1], hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_formatter(data):\n",
    "    formatted_data = []\n",
    "    for i in data.keys():\n",
    "        formatted_data.append({'text': data[i]['text'], 'labels': data[i]['labels']})\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with open('../data/NER_train.json', 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "        train_data = data_formatter(train_data)\n",
    "    with open('../data/NER_test.json', 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "        test_data = data_formatter(test_data)\n",
    "    with open('../data/NER_val.json', 'r') as f:\n",
    "        val_data = json.load(f)\n",
    "        val_data = data_formatter(val_data)\n",
    "        \n",
    "    return train_data, test_data, val_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of entities\n",
    "entities = [\"COURT\",\"PETITIONER\",\"RESPONDENT\",\"JUDGE\",\"DATE\",\"ORG\",\"GPE\",\"STATUTE\",\"PROVISION\",\"PRECEDENT\",\"CASE_NUMBER\",\"WITNESS\",\"OTHER_PERSON\"]\n",
    "\n",
    "# Generate BIO encoding for each entity\n",
    "bio_encoding = []\n",
    "for entity in entities:\n",
    "    bio_encoding.extend([\"B_\" + entity, \"I_\" + entity])\n",
    "\n",
    "bio_encoding.append(\"O\")\n",
    "\n",
    "def label_encoder(labels):\n",
    "    encoded_labels = []\n",
    "    for label in labels:\n",
    "        if label in bio_encoding:\n",
    "            encoded_labels.append(bio_encoding.index(label))\n",
    "        else:\n",
    "            encoded_labels.append(bio_encoding.index(\"O\"))\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset (replace this with your dataset loading code)\n",
    "# Example dataset loading code\n",
    "train_data, test_data, val_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_classes = 27\n",
    "hidden_size = 128\n",
    "output_size = num_classes  # Replace num_classes with the number of output classes\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in validation_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, num_classes), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            predictions = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(validation_loader)\n",
    "    f1_macro = f1_score(all_targets, all_predictions, average='macro')\n",
    "    \n",
    "    return avg_loss, f1_macro\n",
    "\n",
    "def tokenize_inputs(text):\n",
    "    # Remove special characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "    # Tokenize by splitting on whitespace\n",
    "    tokens = text.split(' ')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # This function is used to collate samples into a batch\n",
    "    # Modify it as needed to handle variable-length sequences\n",
    "    \n",
    "    # Get maximum sequence length in the batch\n",
    "    max_length = max(len(sample[0]) for sample in batch)\n",
    "    \n",
    "    # Initialize lists to store padded sequences and corresponding labels\n",
    "    padded_inputs = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    # Pad sequences and adjust labels\n",
    "    for inputs, labels in batch:\n",
    "        # Pad inputs\n",
    "        padded_inputs.append(inputs + \" \" + \"0\" * (max_length - len(inputs)))\n",
    "        \n",
    "        # Pad labels and adjust BIO encoding\n",
    "        padded_labels.append(labels + ['O']) \n",
    "    \n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'high', 'court', 'of', 'calcutta', 'awarded', 'a', 'sum', 'of', 'rs', '48000', 'as', 'compensation']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m---> 31\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Tokenize inputs (replace with your tokenizer)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label_encoder(labels))\u001b[38;5;241m.\u001b[39mto(device)    \u001b[38;5;66;03m# Encode labels (replace with your label encoder)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Iterate over different pre-trained word embeddings\n",
    "for embedding_name, embedding_model in [(\"word2vec\", word2vec), (\"glove\", glove), (\"fasttext\", fasttext)]:\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = []\n",
    "    for word in embedding_model.index_to_key:\n",
    "        embedding_matrix.append(embedding_model[word])\n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "\n",
    "    # Create the model\n",
    "    model = VanillaRNNModel(torch.FloatTensor(embedding_matrix), hidden_size, output_size).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Prepare data loader\n",
    "    train_dataset = SequenceTaggingDataset(train_data)\n",
    "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    val_dataset = SequenceTaggingDataset(val_data)\n",
    "    validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    test_dataset = SequenceTaggingDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = torch.tensor(tokenize_inputs(inputs)).to(device)  # Tokenize inputs (replace with your tokenizer)\n",
    "            labels = torch.tensor(label_encoder(labels)).to(device)    # Encode labels (replace with your label encoder)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.view(-1, output_size), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Evaluation phase\n",
    "        val_loss, val_f1 = evaluate(model, validation_loader, criterion)\n",
    "        \n",
    "        # Average Loss\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # Print loss\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 (macro): {val_f1:.4f}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), f\"vanilla_rnn_{embedding_name}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
