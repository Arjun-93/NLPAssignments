{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Task 1: Part 1__\n",
    "__a.__ First, split the train data of Task_1 into training and validation sets with an 85:15 ratio (randomly\n",
    "stratified).\n",
    "\n",
    "__b.__ Implement a code for BIO (Beginning-Intermediate-Outside) chunking of the given dataset of Task_1\n",
    "(for the three splits). Tokenization should be done based on space, and each token needs to be\n",
    "assigned a BIO label (in format B_label, I_label or O, where “label” refers to one of the 13 legal\n",
    "entities). Preprocessing can be done on the text if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importing essential libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import simplejson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading datasets from JSON__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train data JSON\n",
    "with open('../data/NER_TRAIN_JUDGEMENT.json','r') as f:\n",
    "    train_json = json.load(f)\n",
    "# Loading test data JSON\n",
    "with open('../data/NER_TEST_JUDGEMENT.json','r') as f:\n",
    "    test_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(json_data):\n",
    "    \"\"\"\n",
    "    @brief  This function removes <span> tags from the dataset.\n",
    "    @param  json_data   JSON-extracted dataset.\n",
    "    @return json_data   Processed data as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    for sample in json_data:    # sample-wise processing of data\n",
    "        text = sample['data']['text']   # extract the sample text\n",
    "        processed_text = re.sub(r'<span.*?>|</span>', '', text) # remove HTML tags\n",
    "        sample['data']['text'] = processed_text # overwrite to the sample\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_sequence(json_data):\n",
    "    \"\"\"\n",
    "    @brief  This function removes special sequences from the text data.\n",
    "    @param  json_data   JSON-extracted dataset.\n",
    "    @return json_data   Processed data as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    for sample in json_data:\n",
    "        text = sample['data']['text']\n",
    "        processed_text = re.sub(r'\\n+|\\t+|\\f+','',text) # new-line character\n",
    "        processed_text = re.sub(r'\\u00ad|\\u2013','-',processed_text)    # soft-hyphen and dash\n",
    "        processed_text = re.sub(r'\\u2018|\\u2019',\"'\",processed_text)    # single quotation\n",
    "        processed_text = re.sub(r'\\u20b9','Rs.',processed_text) # Indian Rupee symbol\n",
    "        processed_text = re.sub(r'\\u201e|\\u201f|\\u201c|\\u201d','\\\"',processed_text) # double quotation\n",
    "        processed_text = re.sub(r'\\u00e0','a',processed_text)   # 'a' with accent\n",
    "        processed_text = re.sub(r'\\u00a0',' ',processed_text)   # non-breaking space\n",
    "        sample['data']['text'] = processed_text\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(json_data):\n",
    "    \"\"\"\n",
    "    @brief  This function removes the unwanted extra spaces in text and entity labels.\n",
    "    @param  json_data   JSON-extracted dataset.\n",
    "    @return json_data   Processed data as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    for sample in json_data:\n",
    "        # processing text data of the sample\n",
    "        text = sample['data']['text']\n",
    "        processed_text = re.sub(' +', ' ', text)\n",
    "        sample['data']['text'] = processed_text.strip()\n",
    "        # processing annotation texts of the sample\n",
    "        for annot in sample['annotations'][0]['result']:\n",
    "            entity = annot['value']['text']\n",
    "            processed_entity = re.sub(' +', ' ', entity)\n",
    "            annot['value']['text'] = processed_entity\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing HTML from training data\n",
    "train_json = remove_html(train_json)\n",
    "# Removing HTML from testing data\n",
    "test_json = remove_html(test_json)\n",
    "\n",
    "# Removing special characters from training data\n",
    "train_json = remove_special_sequence(train_json)\n",
    "# Removing special characters from testing data\n",
    "test_json = remove_special_sequence(test_json)\n",
    "\n",
    "# Removing extra spaces from training data\n",
    "train_json = remove_extra_spaces(train_json)\n",
    "# Removing extra spaces from testing data\n",
    "test_json = remove_extra_spaces(test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Splitting NER_TRAIN_JUDGEMENT into training (.85) and validation (.15)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 8020\n",
      "Size of validation data: 1415\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.15 # ratio of validation data to be sampled\n",
    "N_train = len(train_json)   # number of samples in NER_TRAIN_JUDGEMENT.json\n",
    "val_size = int(N_train * validation_split) # number of samples in validation data\n",
    "\n",
    "train_data = [] # initialize training data\n",
    "val_data = random.sample(train_json, val_size)  # randomly sample validation data \n",
    "for sample in train_json:   # add the remaining samples to training data\n",
    "    if sample not in val_data:\n",
    "        train_data.append(sample)\n",
    "\n",
    "print(f'Size of training data: {len(train_data)}')\n",
    "print(f'Size of validation data: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Performing BIO-encoding on TRAIN, VAL and TEST data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOEncoder:\n",
    "    \"\"\"\n",
    "    To perform BIO chunking to TRAIN, VAL, and TEST data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        \"\"\"\n",
    "        @brief  Constructs an instance of BIOEncoder with the given data.\n",
    "        @param  data    Dataset on which encoding is to be performed.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __find_index_range(self, arr, seq, tags):\n",
    "        \"\"\"\n",
    "        @brief  (Inner method) Finds the index range of a sequence of tokens \n",
    "        in the array given the current state of tag assignment. \n",
    "        @param  arr     The list of tokens in which search is needed.\n",
    "        @param  seq     Sequence of tokens to be matched.\n",
    "        @param  tags    Current state of the assigned tags.\n",
    "        @return Tuple of start and end indices of the matched range.\n",
    "        \"\"\"\n",
    "        start_index = -1\n",
    "        end_index = -1\n",
    "        for i in range(len(arr) - len(seq) + 1):\n",
    "            if tags[i] == 'O':  # if the current index is not already marked for a B/I tag\n",
    "                list1 = arr[i:i+len(seq)]   # sequence of tokens from the array\n",
    "                list2 = seq # sequence of tokens to be matched\n",
    "                # to ensure that special characters and 's are eliminated during string matching\n",
    "                regex_pattern = re.compile(r\"[^a-zA-Z0-9\\s']+|'s\\b\") \n",
    "                matched = True  # a flag to keep check on the current match status\n",
    "                for elem1, elem2 in zip(list1, list2):\n",
    "                    match1 = re.sub(regex_pattern, '', elem1)   # remove special chars from elem1\n",
    "                    match2 = re.sub(regex_pattern, '', elem2)   # remove special chars from elem2\n",
    "                    if match1 != match2:    # if not matched, no need to check further\n",
    "                        matched = False\n",
    "                        break\n",
    "                if matched: # if match found, set start and end indices for the range\n",
    "                    start_index = i\n",
    "                    end_index = i + len(seq) - 1\n",
    "                    break\n",
    "        return (start_index, end_index) # return the extreme indices\n",
    "\n",
    "    def __helper_encode(self, text_tokens, entities):\n",
    "        \"\"\"\n",
    "        @brief  (Inner method) Encodes a tokenized text sample for the given \n",
    "        set of entities into BIO tags.\n",
    "        @param  text_tokens Tokenized text sample.\n",
    "        @param  entities    Set of entities to be marked into BIO tags.\n",
    "        @return tags    Sequence of BIO tags from the given text.\n",
    "        \"\"\"\n",
    "        tags = ['O' for _ in range(len(text_tokens))]   # initialize all tags with 'O'\n",
    "        for entity in entities: # perform BIO tagging for each named-entity in the dataset\n",
    "            # find the range of indices for the entity in the tokenized text sample\n",
    "            ent = entity['value']['text'].strip()\n",
    "            e_tokens = list(self.tokenizer(ent))\n",
    "            tokenized_entity = [token.text for token in e_tokens]\n",
    "            rng = self.__find_index_range(text_tokens, tokenized_entity, tags)\n",
    "            start_index = rng[0]\n",
    "            end_index = rng[1]\n",
    "            label = entity['value']['labels'][0]\n",
    "            for idx in range(start_index, end_index+1):\n",
    "                if idx == start_index:  # perform B-tagging\n",
    "                    tags[idx] = 'B_' + label\n",
    "                else:   # perform I-tagging for the remain span ahead of the B-tag\n",
    "                    tags[idx] = 'I_' + label\n",
    "        return tags \n",
    "\n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        @brief  This method performs BIO encoding on the dataset provided to the constructor.\n",
    "        @param  None.\n",
    "        @return tagged_data     A dictionary of dictionaries containing BIO-tagged data.\n",
    "        \"\"\"\n",
    "        tagged_data = dict()\n",
    "        for sample in self.data:    # perform sample-wise tagging\n",
    "            text = sample['data']['text']\n",
    "            tokens = list(self.tokenizer(text)) # tokenize the text sample at every space\n",
    "            tokenized_text = [token.text for token in tokens]\n",
    "            entities = sample['annotations'][0]['result']   # extract the entities to be tagged in the sample\n",
    "            tags = self.__helper_encode(tokenized_text, entities)  # perform BIO-tagging on the sample\n",
    "            tagged_data[sample['id']] = {'text':\" \".join(tokenized_text).strip(),'labels':tags}   # store to tagged_data as needed\n",
    "        return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.blank(\"en\")\n",
    "\n",
    "# Perform BIO-encoding on TRAIN data\n",
    "tagger_train = BIOEncoder(train_data, tokenizer)\n",
    "tagged_train_data = tagger_train.encode()\n",
    "\n",
    "# Perform BIO-encoding on VAL data\n",
    "tagger_val = BIOEncoder(val_data, tokenizer)\n",
    "tagged_val_data = tagger_val.encode()\n",
    "\n",
    "# Perform BIO-encoding on TEST data\n",
    "tagger_test = BIOEncoder(test_json, tokenizer)\n",
    "tagged_test_data = tagger_test.encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Final check to ensure equal lengths of inputs and labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "def run_length_check(data):\n",
    "    for case_id, case_details in data.items():\n",
    "        text_tokens = case_details['text'].split(' ')\n",
    "        labels = case_details['labels']\n",
    "        if len(text_tokens) != len(labels):\n",
    "            print(case_id)\n",
    "            print(case_details['text'])\n",
    "            print(len(case_details['text'].split(' ')))\n",
    "            print(len(case_details['labels']))\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "check_train = run_length_check(tagged_train_data)\n",
    "check_val = run_length_check(tagged_val_data)\n",
    "check_test = run_length_check(tagged_test_data)\n",
    "\n",
    "if check_train and check_val and check_test:\n",
    "    print('SUCCESS!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save processed data as JSON__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TRAIN data\n",
    "with open('../data/NER_train.json','w') as f:\n",
    "    json.dump(tagged_train_data, f, indent=4)\n",
    "# Save VAL data\n",
    "with open('../data/NER_val.json','w') as f:\n",
    "    json.dump(tagged_val_data, f, indent=4)\n",
    "# Save TEST data\n",
    "with open('../data/NER_test.json','w') as f:\n",
    "    json.dump(tagged_test_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
