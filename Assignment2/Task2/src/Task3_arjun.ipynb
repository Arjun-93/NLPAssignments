{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GloVe, FastText, Word2Vec\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "# Load word embeddings\n",
    "word2vec = gensim_api.load(\"word2vec-google-news-300\")\n",
    "glove = gensim_api.load(\"glove-wiki-gigaword-300\")\n",
    "fasttext = gensim_api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word embeddings\n",
    "import gensim.downloader as gensim_api\n",
    "word2vec = gensim_api.load(\"word2vec-google-news-300\")\n",
    "glove = gensim_api.load(\"glove-wiki-gigaword-300\")\n",
    "fasttext = gensim_api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the BiLSTM-CRF model\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tag_to_idx, pretrained_embedding=None):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, len(tag_to_idx))\n",
    "        self.crf = CRF(len(tag_to_idx))\n",
    "\n",
    "    def forward(self, sentence, tags=None):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        if tags is not None:\n",
    "            loss = self.crf(emissions, tags)\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions)\n",
    "\n",
    "# Define the CRF layer\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags):\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "\n",
    "    def forward(self, emissions, tags):\n",
    "        # Compute the CRF loss\n",
    "        pass  # Implement according to the CRF loss formula\n",
    "\n",
    "    def decode(self, emissions):\n",
    "        # Viterbi decoding\n",
    "        pass  # Implement according to the Viterbi decoding algorithm\n",
    "\n",
    "# Convert dataset to PyTorch DataLoader\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, word_to_idx, tag_to_idx):\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = [self.word_to_idx[word] for word in self.data[idx][\"text\"].split()]\n",
    "        tags = [self.tag_to_idx[tag] for tag in self.data[idx][\"labels\"]]\n",
    "        return torch.LongTensor(sentence), torch.LongTensor(tags)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sentence, tags in train_loader:\n",
    "        sentence, tags = sentence.to(device), tags.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(sentence, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in data_loader:\n",
    "            sentence, tags = sentence.to(device), tags.to(device)\n",
    "            loss = model(sentence, tags)\n",
    "            total_loss += loss.item()\n",
    "            predictions = model(sentence)\n",
    "            all_preds.extend(predictions)\n",
    "            all_labels.extend(tags.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(np.concatenate(all_labels), np.concatenate(all_preds), average='macro')\n",
    "    return total_loss / len(data_loader), f1\n",
    "\n",
    "# Training loop\n",
    "def train_loop(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1_scores = []\n",
    "    val_f1_scores = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        val_f1_scores.append(val_f1)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, train_f1_scores, val_f1_scores\n",
    "\n",
    "# Data preprocessing\n",
    "word_to_idx = {}  # Implement word to index mapping\n",
    "tag_to_idx = {\"O\": 0, \"B\": 1, \"I\": 2}  # Implement tag to index mapping\n",
    "embedding_dim = 300  # Set the desired embedding dimension\n",
    "hidden_dim = 128  # Set the desired hidden dimension\n",
    "\n",
    "# Create datasets and loaders for each pre-trained embedding\n",
    "datasets = []\n",
    "loaders = []\n",
    "pretrained_embeddings = [Word2Vec(), GloVe(), FastText()]  # You can specify paths or sizes for these embeddings\n",
    "\n",
    "for embedding in pretrained_embeddings:\n",
    "    dataset = NERDataset(dataset, word_to_idx, tag_to_idx)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    datasets.append(dataset)\n",
    "    loaders.append(loader)\n",
    "\n",
    "# Model training for each pre-trained embedding\n",
    "for i, (dataset, loader) in enumerate(zip(datasets, loaders)):\n",
    "    model = BiLSTM_CRF(embedding_dim, hidden_dim, len(word_to_idx), tag_to_idx, pretrained_embedding=embedding)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()  # You might need to adjust the loss function based on the CRF implementation\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_losses, val_losses, train_f1_scores, val_f1_scores = train_loop(model, loader, loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f'model_{i + 1}.pth')\n",
    "\n",
    "    # Generate plots\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_f1_scores, label='Training F1 Score')\n",
    "    plt.plot(val_f1_scores, label='Validation F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plot_{i + 1}.png')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
