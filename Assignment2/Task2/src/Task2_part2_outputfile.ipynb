{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gensim.downloader as gensim_api\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load word embeddings\n",
    "word2vec = gensim_api.load(\"word2vec-google-news-300\")\n",
    "glove = gensim_api.load(\"glove-wiki-gigaword-300\")\n",
    "fasttext = gensim_api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open('../data/ATE_Train.json','r') as f:\n",
    "    train_json = json.load(f)\n",
    "with open('../data/ATE_Val.json','r') as f:\n",
    "    val_json = json.load(f)\n",
    "with open('../data/ATE_Test.json','r') as f:\n",
    "    test_json = json.load(f)\n",
    "\n",
    "def get_embeddings(text, embeddings):\n",
    "    words = text.split()\n",
    "    embeddings_list = [embeddings[word] if word in embeddings else np.zeros(embeddings.vector_size) for word in words]\n",
    "    return torch.tensor(embeddings_list, dtype=torch.float32)\n",
    "\n",
    "# Convert JSON dataset to numerical format\n",
    "def get_dataset(word_embedding, json_file, label_mapping):\n",
    "    numerical_dataset = []\n",
    "    for key, example in json_file.items():\n",
    "        text_embeddings = get_embeddings(example[\"text\"], word_embedding)\n",
    "        label_sequence = torch.tensor([label_mapping[label] for label in example[\"labels\"]], dtype=torch.float32)\n",
    "        numerical_dataset.append((text_embeddings, label_sequence))\n",
    "    return numerical_dataset\n",
    "\n",
    "label_mapping = {\"B\": 0, \"I\": 1, \"O\": 2}\n",
    "word2vec_train_embeddings = get_dataset(word_embedding=word2vec, json_file=train_json, label_mapping=label_mapping)\n",
    "word2vec_val_embeddings = get_dataset(word_embedding=word2vec, json_file=val_json, label_mapping=label_mapping)\n",
    "word2vec_test_embeddings = get_dataset(word_embedding=word2vec, json_file=test_json, label_mapping=label_mapping)\n",
    "\n",
    "glove_train_embeddings = get_dataset(word_embedding=glove, json_file=train_json, label_mapping=label_mapping)\n",
    "glove_val_embeddings = get_dataset(word_embedding=glove, json_file=val_json, label_mapping=label_mapping)\n",
    "glove_test_embeddings = get_dataset(word_embedding=glove, json_file=test_json, label_mapping=label_mapping)\n",
    "\n",
    "fasttext_train_embeddings = get_dataset(word_embedding=fasttext, json_file=train_json, label_mapping=label_mapping)\n",
    "fasttext_val_embeddings = get_dataset(word_embedding=fasttext, json_file=val_json, label_mapping=label_mapping)\n",
    "fasttext_test_embeddings = get_dataset(word_embedding=fasttext, json_file=test_json, label_mapping=label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # Initialize hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = 1\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, self.n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(x.device)  # Initialize hidden state\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(x.device)  # Initialize cell state\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))  # Forward propagate LSTM\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # Initialize hidden state\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Forward pass\n",
    "            inputs = batch['features']\n",
    "            outputs = model(inputs)\n",
    "            # Process outputs and convert to predictions\n",
    "\n",
    "    # Flatten predictions and true labels for computing metrics\n",
    "    all_predictions = [p for sublist in all_predictions for p in sublist]\n",
    "    all_true_labels = [l for sublist in all_true_labels for l in sublist]\n",
    "\n",
    "    # Compute accuracy and F1 score\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    f1 = f1_score(all_true_labels, all_predictions, average='macro')\n",
    "\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'lstm': LSTMModel,\n",
    "    'gru': GRUModel,\n",
    "    'vanilla_rnn': VanillaRNN\n",
    "}\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 300\n",
    "hidden_size = 128\n",
    "output_size = 3  # Number of classes (BIO tags)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = VanillaRNN(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load(\"vanilla_rnn_word2vec_model.pth\"))\n",
    "\n",
    "# Calculate and print the test F1 score\n",
    "test_f1_score = evaluate_model(model, word2vec_test_embeddings)\n",
    "print(f'{model_name} Model Testing using word2vec embeddings (Test F1 Score): {test_f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
