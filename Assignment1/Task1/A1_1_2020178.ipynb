{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement FROM SCRATCH a tokenizer based on the BytePair encoding algorithm (link). You\n",
    "are only allowed to use standard Python libraries and objects (lists, arrays, dictionaries, and\n",
    "collections library). Use of existing frameworks (such as nltk, HuggingFace, Spacy, TextBlob) is\n",
    "not allowed.\n",
    "</n>\n",
    "<br>\n",
    "Specifically, you are required to create a Tokenizer class in python, which implements the\n",
    "following methods:\n",
    "* learn_vocablury(), which takes as parameter the corpus number of merges and learns\n",
    "the split rules and frequencies; and </n>\n",
    "* tokenize(), which takes as input a sample and tokenizes it based on the learnt rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S|\\$)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_vocab(data):\n",
    "    vocab = defaultdict(int)\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            token = ' '.join(list(word)) + ' $'\n",
    "            vocab[token] += 1\n",
    "    return vocab\n",
    "\n",
    "def byte_pair_encoding(data, n, vocab_output_file, merge_rule_output_file, tokenized_samples_output_file):\n",
    "    vocab = get_vocab(data)\n",
    "\n",
    "    # Write all possible tokens in the vocabulary to the file\n",
    "    with open(vocab_output_file, 'w') as f:\n",
    "        for token in vocab:\n",
    "            f.write(token + '\\n')\n",
    "\n",
    "    for i in range(n):\n",
    "        pairs = get_stats(vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "\n",
    "        # Write merge rules to the file\n",
    "        with open(merge_rule_output_file, 'a') as f_merge:\n",
    "            f_merge.write(','.join(best) + '\\n')\n",
    "\n",
    "        # Write tokens after each iteration to the file\n",
    "        with open(tokenized_samples_output_file, 'a') as f_samples:\n",
    "            for token in vocab:\n",
    "                f_samples.write(token.replace(' ', ',') + '\\n')\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# read corpus from text file\n",
    "with open('corpus.txt', 'r') as file:\n",
    "    data = file.read().splitlines()\n",
    "    \n",
    "n = 100\n",
    "vocab_output_file = 'all_possible_tokens.txt'\n",
    "merge_rule_output_file = 'merge_rules.txt'\n",
    "tokenized_samples_output_file = 'tokenized_samples.txt'\n",
    "\n",
    "open(vocab_output_file, 'w').close()\n",
    "open(merge_rule_output_file, 'w').close()\n",
    "open(tokenized_samples_output_file, 'w').close()\n",
    "\n",
    "byte_pair_encoding(data, n, vocab_output_file, merge_rule_output_file, tokenized_samples_output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
