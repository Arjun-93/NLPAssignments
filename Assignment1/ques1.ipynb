{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement FROM SCRATCH a tokenizer based on the BytePair encoding algorithm (link). You\n",
    "are only allowed to use standard Python libraries and objects (lists, arrays, dictionaries, and\n",
    "collections library). Use of existing frameworks (such as nltk, HuggingFace, Spacy, TextBlob) is\n",
    "not allowed.\n",
    "</n>\n",
    "<br>\n",
    "Specifically, you are required to create a Tokenizer class in python, which implements the\n",
    "following methods:\n",
    "* learn_vocablury(), which takes as parameter the corpus number of merges and learns\n",
    "the split rules and frequencies; and </n>\n",
    "* tokenize(), which takes as input a sample and tokenizes it based on the learnt rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_vocab(text):\n",
    "    vocab = Counter(text.split())\n",
    "    return {' '.join(word): freq for word, freq in vocab.items()}\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "def tokenize_samples(samples, vocab):\n",
    "    tokens_per_sample = []\n",
    "    for sample in samples:\n",
    "        tokens = sample.split()\n",
    "        for i in range(len(tokens)):\n",
    "            if i < len(tokens) - 1:\n",
    "                pair = (tokens[i], tokens[i+1])\n",
    "                if pair in vocab:\n",
    "                    tokens[i] = ''.join(pair) + '$'\n",
    "                    tokens[i+1] = ''\n",
    "        tokens_per_sample.append([token for token in tokens if token]) \n",
    "    return tokens_per_sample\n",
    "\n",
    "# Read from corpus.txt\n",
    "with open(\"corpus.txt\", \"r\") as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "\n",
    "vocab = get_vocab(text)\n",
    "# print(\"Initial vocabulary:\", vocab)\n",
    "num_merges = 5  \n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best_pair, vocab)\n",
    "\n",
    "# Save results back to corpus.txt\n",
    "with open(\"corpus.txt\", \"w\") as corpus_file:\n",
    "    corpus_file.write(text)  # Writing the original text\n",
    "    for token in vocab.keys():\n",
    "        corpus_file.write('\\n' + ' '.join(list(token.replace(' ', '$'))))  # Writing tokens with $ separator\n",
    "\n",
    "# Save all possible tokens in the vocabulary to tokens.txt\n",
    "with open(\"tokens.txt\", \"w\") as tokens_file:\n",
    "    for token in vocab.keys():\n",
    "        tokens = list(token.replace(' ', '$'))\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(i, len(tokens)):\n",
    "                tokens_file.write(''.join(tokens[i:j+1]) + '\\n')\n",
    "\n",
    "\n",
    "# Save all the merge rules to merge_rules.txt\n",
    "with open(\"merge_rules.txt\", \"w\") as rules_file:\n",
    "    for rule in vocab.keys():\n",
    "        rules_file.write(rule.replace(' ', ',') + '\\n')\n",
    "\n",
    "# Tokenize a set of test samples and write the tokens to tokenized_samples.txt\n",
    "test_samples = [\"low widest\", \"newest lower\"]\n",
    "tokenized_samples = tokenize_samples(test_samples, vocab)\n",
    "\n",
    "with open(\"tokenized_samples.txt\", \"w\") as sample_file:\n",
    "    for tokens in tokenized_samples:\n",
    "        sample_file.write(','.join(tokens) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM():\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.bigram_counts = self.get_bigram_counts()\n",
    "        self.unigram_counts = self.get_unigram_counts()\n",
    "        self.bigram_probs = self.get_bigram_probs()\n",
    "        self.unigram_probs = self.get_unigram_probs()\n",
    "        self.bigram_laplace_probs = self.get_bigram_laplace_probs()\n",
    "        self.unigram_laplace_probs = self.get_unigram_laplace_probs()\n",
    "\n",
    "    def get_bigram_counts(self):\n",
    "        bigram_counts = defaultdict(int)\n",
    "        for i in range(len(self.tokens) - 1):\n",
    "            bigram_counts[self.tokens[i], self.tokens[i+1]] += 1\n",
    "        return bigram_counts\n",
    "\n",
    "    def get_unigram_counts(self):\n",
    "        unigram_counts = defaultdict(int)\n",
    "        for token in self.tokens:\n",
    "            unigram_counts[token] += 1\n",
    "        return unigram_counts\n",
    "\n",
    "    def get_bigram_probs(self):\n",
    "        bigram_probs = defaultdict(int)\n",
    "        for bigram, count in self.bigram_counts.items():\n",
    "            bigram_probs[bigram] = count / self.unigram_counts[bigram[0]]\n",
    "        return bigram_probs\n",
    "\n",
    "    def get_unigram_probs(self):\n",
    "        unigram_probs = defaultdict(int)\n",
    "        for unigram, count in self.unigram_counts.items():\n",
    "            unigram_probs[unigram] = count / len(self.tokens)\n",
    "        return unigram_probs\n",
    "\n",
    "    def get_bigram_laplace_probs(self):\n",
    "        bigram_laplace_probs = defaultdict(int)\n",
    "        for bigram, count in self.bigram_counts.items():\n",
    "            bigram_laplace_probs[bigram] = (count + 1) / (self.unigram_counts[bigram[0]] + len(self.unigram_counts))\n",
    "        return bigram_laplace_probs\n",
    "\n",
    "    def get_unigram_laplace_probs(self):\n",
    "        unigram_laplace_probs = defaultdict(int)\n",
    "        for unigram, count in self.unigram_counts.items():\n",
    "            unigram_laplace_probs[unigram] = (count + 1) / (len(self.tokens) + len(self.unigram_counts))\n",
    "        return unigram_laplace_probs\n",
    "    \n",
    "    def get_bigram_interpolated_probs(self):\n",
    "        bigram_interpolated_probs = defaultdict(int)\n",
    "        for bigram, count in self.bigram_counts.items():\n",
    "            bigram_interpolated_probs[bigram] = 0.5 * (count / self.unigram_counts[bigram[0]]) + 0.5 * (self.unigram_counts[bigram[1]] / len(self.tokens))\n",
    "        return bigram_interpolated_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
